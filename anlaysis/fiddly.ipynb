{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter, defaultdict as ddict\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"C:/Users/ka021/Documents/PopSPanCancerAnalysis Progress Summary.txt\") as inp:\n",
    "    text = inp.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [p.strip('.,();:').lower() for p in re.split(\"[\\s-]\", text)]\n",
    "words = [w for w in words if len(w) > 0]\n",
    "word_count = Counter(words)\n",
    "count_words = ddict(list)\n",
    "for w, c in word_count.items():\n",
    "    count_words[c].append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"as\",\n",
      "\"various\",\n",
      "\"biobanks\",\n",
      "\"variants\",\n",
      "\"drivers\",\n",
      "\"demographics\",\n",
      "\"associations\",\n",
      "\"stress\",\n",
      "\"exomes\",\n",
      "\"has\",\n",
      "\"algorithms\",\n",
      "\"series\",\n",
      "\"scripts\",\n",
      "\"costs\",\n",
      "\"points\",\n",
      "\"genotypes\",\n",
      "\"gvcfs\",\n",
      "\"is\",\n",
      "\"practices\",\n",
      "\"glnexus\",\n",
      "\"analysis\",\n",
      "\"dimensions\",\n"
     ]
    }
   ],
   "source": [
    "for w in word_count:\n",
    "    if w.endswith('s'): print(f'\"{w}\",')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "plurals = [\n",
    "    \"biobanks\",\n",
    "    \"variants\",\n",
    "    \"drivers\",\n",
    "    \"associations\",\n",
    "    \"drivers\",\n",
    "    \"exomes\",\n",
    "    \"algorithms\",\n",
    "    \"scripts\",\n",
    "    \"genotypes\",\n",
    "    \"gvcfs\",\n",
    "    \"practices\",\n",
    "    \"dimensions\",\n",
    "    \"points\",\n",
    "    \"costs\",\n",
    "    \"demographics\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple check for standard english plurals (-s suffix)\n",
    "words_singular = [w[:-1] if w in plurals else w for w in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median 5.0 : cc 110\n",
      "Under cc 323\n",
      "Over cc 872\n"
     ]
    }
   ],
   "source": [
    "ls = np.array([len(w) for w in words_singular])\n",
    "\n",
    "med = np.median(ls)\n",
    "print('Median', med, ': cc', np.sum(ls[ls == med]))\n",
    "print('Under cc', np.sum(ls[ls < med]))\n",
    "print('Over cc', np.sum(ls[ls > med]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count_s = Counter(words_singular)\n",
    "count_words_s = ddict(list)\n",
    "for w, c in word_count_s.items():\n",
    "    count_words_s[c].append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"as\",\n",
      "\"various\",\n",
      "\"stress\",\n",
      "\"has\",\n",
      "\"series\",\n",
      "\"is\",\n",
      "\"glnexus\",\n",
      "\"analysis\",\n"
     ]
    }
   ],
   "source": [
    "for w in word_count_s:\n",
    "    if w.endswith('s'): print(f'\"{w}\",')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['we', 'cancer']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_words_s[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 sequencing, enrichment, according, genotype, various, biobank, better, while, much, form, with, gvcf, two, an, a\n",
      "3 exome, are\n",
      "4 cancer, we\n",
      "5 for, in, as\n",
      "6 of\n",
      "7 data\n",
      "10 and\n",
      "11 to\n",
      "12 the\n"
     ]
    }
   ],
   "source": [
    "maxl = max(count_words_s.keys())\n",
    "\n",
    "for i in range(2,maxl+1):\n",
    "    if not i in count_words_s: continue\n",
    "\n",
    "    print(i, \", \".join(sorted(count_words_s[i], key=lambda x: -len(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disproportionately stratification unprecedented \n",
      "consideration relationship heritability incorporated \n",
      "increasingly compromising partitioning statistical \n",
      "identifying genetically underserved demographic association \n",
      "aggregating methodology deepvariant maintaining associated \n",
      "pathogenic particular understand especially management \n",
      "performant analytical aggregated refinement conversion \n",
      "stratified population aggregate providing examining \n",
      "discovery optimized algorithm developed institute currently \n",
      "designing potential dimension maximize possible stratify \n",
      "inferred ancestry burdened familiar required pipeline \n",
      "practice analysis variant between genetic history insight \n",
      "provide context million careful aligned glnexus further \n",
      "effort driver needed expect reveal clutch stress cohort \n",
      "volume regard series script called before power order often \n",
      "serve first major reach we've depth these entry point broad \n",
      "using phase again other from want into will both test what \n",
      "many task such that time cost wide hg38 best type over aim \n",
      "new may has end wdl raw uk is by "
     ]
    }
   ],
   "source": [
    "by_size = sorted(count_words_s[1], key=lambda x: -len(x))\n",
    "\n",
    "W = 60\n",
    "c = 0\n",
    "for w in by_size:\n",
    "    if c + len(w) > 60:\n",
    "        print()\n",
    "        c = 0\n",
    "    print(w, end=' ')\n",
    "    c += len(w) + 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "common = [\"the\", \"an\", \"a\", \"as\", \"to\", \"of\", \"and\", \"then\", \"in\", \"for\", \"we\", \"are\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(167, 6.868263473053892)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N, S = 0, 0\n",
    "\n",
    "for w, c in word_count_s.items():\n",
    "    if w in common: continue\n",
    "    N += c\n",
    "    S += c * len(w)\n",
    "\n",
    "N, S/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "things = \\\n",
    "['spark.app.id',\n",
    " 'spark.app.initial.jar.urls',\n",
    " 'spark.app.name',\n",
    " 'spark.app.startTime',\n",
    " 'spark.app.submitTime',\n",
    " 'spark.checkpoint.compress',\n",
    " 'spark.dataproc.metrics.listener.metrics.collector.hostname',\n",
    " 'spark.dataproc.sql.joinConditionReorder.enabled',\n",
    " 'spark.dataproc.sql.local.rank.pushdown.enabled',\n",
    " 'spark.dataproc.sql.optimizer.leftsemijoin.conversion.enabled',\n",
    " 'spark.dataproc.sql.parquet.enableFooterCache',\n",
    " 'spark.driver.appUIAddress',\n",
    " 'spark.driver.extraClassPath',\n",
    " 'spark.driver.extraJavaOptions',\n",
    " 'spark.driver.host',\n",
    " 'spark.driver.maxResultSize',\n",
    " 'spark.driver.memory',\n",
    " 'spark.driver.port',\n",
    " 'spark.dynamicAllocation.enabled',\n",
    " 'spark.dynamicAllocation.maxExecutors',\n",
    " 'spark.dynamicAllocation.minExecutors',\n",
    " 'spark.eventLog.dir',\n",
    " 'spark.eventLog.enabled',\n",
    " 'spark.executor.cores',\n",
    " 'spark.executor.extraClassPath',\n",
    " 'spark.executor.extraJavaOptions',\n",
    " 'spark.executor.id',\n",
    " 'spark.executor.instances',\n",
    " 'spark.executor.memory',\n",
    " 'spark.executorEnv.OPENBLAS_NUM_THREADS',\n",
    " 'spark.executorEnv.PYTHONPATH',\n",
    " 'spark.extraListeners',\n",
    " 'spark.hadoop.fs.gs.requester.pays.mode',\n",
    " 'spark.hadoop.fs.gs.requester.pays.project.id',\n",
    " 'spark.hadoop.hive.execution.engine',\n",
    " 'spark.hadoop.io.compression.codecs',\n",
    " 'spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version',\n",
    " 'spark.hadoop.mapreduce.fileoutputcommitter.concurrent.write.enabled',\n",
    " 'spark.hadoop.mapreduce.input.fileinputformat.split.minsize',\n",
    " 'spark.history.fs.logDirectory',\n",
    " 'spark.jars',\n",
    " 'spark.kryo.registrator',\n",
    " 'spark.kryoserializer.buffer.max',\n",
    " 'spark.logConf',\n",
    " 'spark.master',\n",
    " 'spark.metrics.namespace',\n",
    " 'spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_HOSTS',\n",
    " 'spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_URI_BASES',\n",
    " 'spark.repl.local.jars',\n",
    " 'spark.rpc.message.maxSize',\n",
    " 'spark.scheduler.minRegisteredResourcesRatio',\n",
    " 'spark.scheduler.mode',\n",
    " 'spark.serializer',\n",
    " 'spark.shuffle.service.enabled',\n",
    " 'spark.sql.adaptive.enabled',\n",
    " 'spark.sql.autoBroadcastJoinThreshold',\n",
    " 'spark.sql.catalogImplementation',\n",
    " 'spark.sql.cbo.enabled',\n",
    " 'spark.sql.cbo.joinReorder.enabled',\n",
    " 'spark.sql.parquet.enableNestedColumnVectorizedReader',\n",
    " 'spark.submit.deployMode',\n",
    " 'spark.submit.pyFiles',\n",
    " 'spark.ui.filters',\n",
    " 'spark.ui.port',\n",
    " 'spark.ui.proxyBase',\n",
    " 'spark.ui.showConsoleProgress',\n",
    " 'spark.yarn.am.memory',\n",
    " 'spark.yarn.dist.jars',\n",
    " 'spark.yarn.historyServer.address',\n",
    " 'spark.yarn.isPython',\n",
    " 'spark.yarn.jars',\n",
    " 'spark.yarn.secondary.jars',\n",
    " 'spark.yarn.unmanagedAM.enabled']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = list(zip(things, [1] * len(things)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def branchy_tree(paths, prefix=\"\"):\n",
    "    if len(paths) == 1: return {paths[0][0]: paths[0][1]}\n",
    "    \n",
    "    # find greatest common prefix\n",
    "    ps = paths[0][0].split('.')\n",
    "    gcp = \"\"\n",
    "    i = 0\n",
    "\n",
    "    while i < len(ps):\n",
    "        ncp = gcp + ps[i]\n",
    "\n",
    "        if all(p.startswith(ncp) for p, q in paths):\n",
    "            gcp = ncp + \".\"\n",
    "            i += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    if i == len(ps):\n",
    "        raise ValueError(f'Non-unique path encountered: {ncp}')\n",
    "\n",
    "    tree = dict()\n",
    "    paths = [(p.removeprefix(gcp), q) for p, q in paths]\n",
    "    while len(paths) > 0:\n",
    "        apre = paths[0][0].split('.')[0]\n",
    "        agrp = [p for p in paths if p[0].startswith(apre)]\n",
    "        for a in agrp: paths.remove(a)\n",
    "\n",
    "        tree |= branchy_tree(agrp, gcp)\n",
    "        \n",
    "    return {gcp[:-1]: tree}\n",
    "\n",
    "\n",
    "def display_tree(tree, i=0):\n",
    "    for k, v in tree.items():\n",
    "        print(\" |\"*i + \"-\" + str(k), end='')\n",
    "        if type(v) == dict:\n",
    "            print()\n",
    "            display_tree(v, i+1)\n",
    "        else:\n",
    "            print(\" = \" + str(v))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"-\".join([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-spark\n",
      " |-app\n",
      " | |-id = 1\n",
      " | |-initial.jar.urls = 1\n",
      " | |-name = 1\n",
      " | |-startTime = 1\n",
      " | |-submitTime = 1\n",
      " |-checkpoint.compress = 1\n",
      " |-dataproc\n",
      " | |-metrics.listener.metrics.collector.hostname = 1\n",
      " | |-sql\n",
      " | | |-joinConditionReorder.enabled = 1\n",
      " | | |-local.rank.pushdown.enabled = 1\n",
      " | | |-optimizer.leftsemijoin.conversion.enabled = 1\n",
      " | | |-parquet.enableFooterCache = 1\n",
      " |-driver\n",
      " | |-appUIAddress = 1\n",
      " | |-extraClassPath = 1\n",
      " | |-extraJavaOptions = 1\n",
      " | |-host = 1\n",
      " | |-maxResultSize = 1\n",
      " | |-memory = 1\n",
      " | |-port = 1\n",
      " |-dynamicAllocation\n",
      " | |-enabled = 1\n",
      " | |-maxExecutors = 1\n",
      " | |-minExecutors = 1\n",
      " |-eventLog\n",
      " | |-dir = 1\n",
      " | |-enabled = 1\n",
      " |-executor\n",
      " | |-cores = 1\n",
      " | |-extraClassPath = 1\n",
      " | |-extraJavaOptions = 1\n",
      " | |-id = 1\n",
      " | |-instances = 1\n",
      " | |-memory = 1\n",
      " | |-executorEnv\n",
      " | | |-OPENBLAS_NUM_THREADS = 1\n",
      " | | |-PYTHONPATH = 1\n",
      " |-extraListeners = 1\n",
      " |-hadoop\n",
      " | |-fs.gs.requester.pays\n",
      " | | |-mode = 1\n",
      " | | |-project.id = 1\n",
      " | |-hive.execution.engine = 1\n",
      " | |-io.compression.codecs = 1\n",
      " | |-mapreduce\n",
      " | | |-fileoutputcommitter\n",
      " | | | |-algorithm.version = 1\n",
      " | | | |-concurrent.write.enabled = 1\n",
      " | | |-input.fileinputformat.split.minsize = 1\n",
      " |-history.fs.logDirectory = 1\n",
      " |-jars = 1\n",
      " |-kryo\n",
      " | |-registrator = 1\n",
      " | |-kryoserializer.buffer.max = 1\n",
      " |-logConf = 1\n",
      " |-master = 1\n",
      " |-metrics.namespace = 1\n",
      " |-org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param\n",
      " | |-PROXY_HOSTS = 1\n",
      " | |-PROXY_URI_BASES = 1\n",
      " |-repl.local.jars = 1\n",
      " |-rpc.message.maxSize = 1\n",
      " |-scheduler\n",
      " | |-minRegisteredResourcesRatio = 1\n",
      " | |-mode = 1\n",
      " |-serializer = 1\n",
      " |-shuffle.service.enabled = 1\n",
      " |-sql\n",
      " | |-adaptive.enabled = 1\n",
      " | |-autoBroadcastJoinThreshold = 1\n",
      " | |-catalogImplementation = 1\n",
      " | |-cbo\n",
      " | | |-enabled = 1\n",
      " | | |-joinReorder.enabled = 1\n",
      " | |-parquet.enableNestedColumnVectorizedReader = 1\n",
      " |-submit\n",
      " | |-deployMode = 1\n",
      " | |-pyFiles = 1\n",
      " |-ui\n",
      " | |-filters = 1\n",
      " | |-port = 1\n",
      " | |-proxyBase = 1\n",
      " | |-showConsoleProgress = 1\n",
      " |-yarn\n",
      " | |-am.memory = 1\n",
      " | |-dist.jars = 1\n",
      " | |-historyServer.address = 1\n",
      " | |-isPython = 1\n",
      " | |-jars = 1\n",
      " | |-secondary.jars = 1\n",
      " | |-unmanagedAM.enabled = 1\n"
     ]
    }
   ],
   "source": [
    "tree = branchy_tree(paths)\n",
    "display_tree(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import ArgumentParser\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--spark-conf'], dest='spark_conf', nargs=None, const=None, default='spark.executor.memory=2g', type=None, choices=None, required=False, help='spark configuration options as <option>=<value>', metavar=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clap = ArgumentParser(prog=\"Hail PCA wrapper\",\n",
    "                      description=\"Convenience wrapper for performing hail PCA\")\n",
    "clap.add_argument('-r', '--reference', default='GRCh38', choices=['GRCh37', 'GRCh38', 'GRCm38', 'CanFam3'],\n",
    "                  help='the (hail-supported) reference genome of the samples')\n",
    "clap.add_argument('-b', '--bucket', default=os.environ.get('WORKSPACE_BUCKET', None),\n",
    "                  help='cloud bucket prefix to use for saving hail files')\n",
    "clap.add_argument('--spark-conf', default=\"spark.executor.memory=2g\",\n",
    "                  help='spark configuration options as <option>=<value>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': '2', 'b': '3'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = \"a=2 b=3\"\n",
    "\n",
    "dict(pair.split('=') for pair in args.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cline = '##contig=<ID=20,length=62435964,assembly=B36,md5=f126cdf8a6e0c7f379d618ff66beb2da,species=\"Homo sapiens\",taxonomy=x>'\n",
    "re.search('ID=([^,]+),', cline)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "import json\n",
    "dl = 'C:\\\\Users\\\\ka021\\\\Downloads'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(join(dl, 'outputs.json')) as inp:\n",
    "    cohort_conf = json.loads(inp.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(join(dl, 'outputs (1).json')) as inp:\n",
    "    case_conf = json.loads(inp.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'annotated_intervals',\n",
       "  'calling_configs',\n",
       "  'contig_ploidy_calls_tar',\n",
       "  'contig_ploidy_calls_tar_path_list',\n",
       "  'contig_ploidy_model_tar',\n",
       "  'denoising_configs',\n",
       "  'filtered_intervals',\n",
       "  'gcnv_calls_tars_path_list',\n",
       "  'gcnv_model_tars',\n",
       "  'gcnvkernel_version',\n",
       "  'genotyped_intervals_vcf_indexes_path_list',\n",
       "  'genotyped_intervals_vcfs_path_list',\n",
       "  'genotyped_segments_vcf_indexes_path_list',\n",
       "  'genotyped_segments_vcfs_path_list',\n",
       "  'model_qc_status_file',\n",
       "  'model_qc_string',\n",
       "  'read_counts_entity_ids',\n",
       "  'sample_qc_status_files',\n",
       "  'sample_qc_status_strings',\n",
       "  'sharded_interval_lists'},\n",
       " {'qc_status_files', 'qc_status_strings', 'read_counts_entity_id'},\n",
       " {'denoised_copy_ratios',\n",
       "  'gcnv_calls_tars',\n",
       "  'gcnv_tracking_tars',\n",
       "  'genotyped_intervals_vcf_indexes',\n",
       "  'genotyped_intervals_vcfs',\n",
       "  'genotyped_segments_vcf_indexes',\n",
       "  'genotyped_segments_vcfs',\n",
       "  'preprocessed_intervals',\n",
       "  'read_counts',\n",
       "  'sample_contig_ploidy_calls_tars'})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chk = set(k.removeprefix('CNVGermlineCohortWorkflow.') for k in cohort_conf)\n",
    "csk = set(k.removeprefix('CNVGermlineCaseWorkflow.') for k in case_conf)\n",
    "\n",
    "chk - csk, csk - chk, chk & csk"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
